{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Analysis for CWE Classification (zero-shot and few-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_df_cwe(base_dir):\n",
    "    # Iterate through the folders and files\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.startswith('cwe_classification') and file.endswith('.xlsx'):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    \n",
    "                # Read the file into each data frame\n",
    "                if \"gpt35\" in file:\n",
    "                    print(\"reading file in gpt35:\", file_path)\n",
    "                    df_gpt35 = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_gpt35['true_label_binary'] = df_gpt35['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"gpt4\" in file:\n",
    "                    print(\"reading file in gpt4:\", file_path)\n",
    "                    df_gpt4 = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_gpt4['true_label_binary'] = df_gpt4['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"gpt-4o\" in file:\n",
    "                    print(\"reading file in gpt-4o:\", file_path)\n",
    "                    df_gpt4o = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_gpt4o['true_label_binary'] = df_gpt4o['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"CodeLlama-7b\" in file:\n",
    "                    print(\"reading file in CodeLlama-7b:\", file_path)\n",
    "                    df_codellama7b = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_codellama7b['true_label_binary'] = df_codellama7b['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"CodeLlama-13b\" in file:\n",
    "                    print(\"reading file in CodeLlama-13b:\", file_path)\n",
    "                    df_codellama13b = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_codellama13b['true_label_binary'] = df_codellama13b['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"text-bison\" in file:\n",
    "                    print(\"reading filein palm2:\", file_path)\n",
    "                    df_palm2 = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_palm2['true_label_binary'] = df_palm2['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"gemini\" in file:\n",
    "                    print(\"reading file in gemini:\", file_path)\n",
    "                    df_gemini = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_gemini['true_label_binary'] = df_gemini['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                    # Rename the column `vul_binary_classifications` to `vul_file_class\n",
    "                    df_gemini.rename(columns={'vul_binary_classification': 'vul_file_class'}, inplace=True)\n",
    "\n",
    "    return df_gpt35, df_gpt4, df_gpt4o, df_codellama7b, df_codellama13b, df_gemini\n",
    "\n",
    "def load_data_to_df_cwe_few(base_dir):\n",
    "    # Iterate through the folders and files\n",
    "    for folder in os.listdir(base_dir):\n",
    "        folder_path = os.path.join(base_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file in os.listdir(folder_path):\n",
    "                if file.startswith('cwe_few_shot_classification') and file.endswith('.xlsx'):\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    \n",
    "                # Read the file into each data frame\n",
    "                if \"gpt35\" in file:\n",
    "                    print(\"reading file in gpt35:\", file_path)\n",
    "                    df_gpt35 = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_gpt35['true_label_binary'] = df_gpt35['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"gpt4\" in file:\n",
    "                    print(\"reading file in gpt4:\", file_path)\n",
    "                    df_gpt4 = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_gpt4['true_label_binary'] = df_gpt4['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"gpt-4o\" in file:\n",
    "                    print(\"reading file in gpt-4o:\", file_path)\n",
    "                    df_gpt4o = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_gpt4o['true_label_binary'] = df_gpt4o['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"CodeLlama-7b\" in file:\n",
    "                    print(\"reading file in CodeLlama-7b:\", file_path)\n",
    "                    df_codellama7b = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_codellama7b['true_label_binary'] = df_codellama7b['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"CodeLlama-13b\" in file:\n",
    "                    print(\"reading file in CodeLlama-13b:\", file_path)\n",
    "                    df_codellama13b = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_codellama13b['true_label_binary'] = df_codellama13b['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"text-bison\" in file:\n",
    "                    print(\"reading filein palm2:\", file_path)\n",
    "                    df_palm2 = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_palm2['true_label_binary'] = df_palm2['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                elif \"gemini\" in file:\n",
    "                    print(\"reading file in gemini:\", file_path)\n",
    "                    df_gemini = pd.read_excel(file_path)\n",
    "                    # Assign the new column: `true_label_binary` to the data frame\n",
    "                    df_gemini['true_label_binary'] = df_gemini['true_label'].apply(lambda x: 'vulnerable' if x != 'non-vul' else 'not vulnerable')\n",
    "                    # Rename the column `vul_binary_classifications` to `vul_file_class\n",
    "                    df_gemini.rename(columns={'vul_binary_classification': 'vul_file_class'}, inplace=True)\n",
    "\n",
    "    return df_gpt35, df_gpt4, df_gpt4o, df_codellama7b, df_codellama13b, df_gemini\n",
    "\n",
    "\n",
    "# Helper function to calculate the metrics\n",
    "def calculate_metrics(df, metrics_dict, model_name, output=False):\n",
    "    # Assuming your DataFrame is named 'df'\n",
    "    y_true = df['true_label']\n",
    "    y_pred = df['vul_file_class']\n",
    "\n",
    "    # Compute accuracy and round to two decimal places\n",
    "    accuracy = round(accuracy_score(y_true, y_pred), 2)\n",
    "\n",
    "    # Compute precision and round to two decimal places\n",
    "    precision = round(precision_score(y_true, y_pred, average='macro'), 2)\n",
    "\n",
    "    # Compute recall and round to two decimal places\n",
    "    recall = round(recall_score(y_true, y_pred, average='macro'), 2)\n",
    "\n",
    "    # Compute F1-score and round to two decimal places\n",
    "    f1 = round(f1_score(y_true, y_pred, average='macro'), 2)\n",
    "\n",
    "    if output:\n",
    "        # Print the metrics\n",
    "        print(f\"Metrics for {model_name}:\")\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "        print(\"F1-score:\", f1)\n",
    "\n",
    "    # Append the metrics to the list in the dictionary\n",
    "    metrics_dict[model_name].append(accuracy)\n",
    "    metrics_dict[model_name].append(precision)\n",
    "    metrics_dict[model_name].append(recall)\n",
    "    metrics_dict[model_name].append(f1)\n",
    "\n",
    "\n",
    "def calculate_metrics_per_language  (df, model_name, metrics_dict, output=False):\n",
    "   # Assuming your DataFrame is named 'df'\n",
    "    languages = df['language'].unique()\n",
    "    if output:\n",
    "        print(f\"Result for {model_name}:\")\n",
    "\n",
    "    for language in languages:\n",
    "        if output:\n",
    "            print(f\"Metrics for language: {language}\")\n",
    "        # Create a temporary list to store the metrics for the current language\n",
    "        metrics = []\n",
    "\n",
    "        # Filter the DataFrame for the current language\n",
    "        df_language = df[df['language'] == language]\n",
    "        \n",
    "        # Get the true labels and predicted labels for the current language\n",
    "        y_true = df_language['true_label']\n",
    "        y_pred = df_language['vul_file_class']\n",
    "        \n",
    "        # Compute accuracy and round to two decimal places\n",
    "        accuracy = float(round(accuracy_score(y_true, y_pred), 2))\n",
    "\n",
    "        # Compute precision and round to two decimal places\n",
    "        precision = float(round(precision_score(y_true, y_pred, average='macro'), 2))\n",
    "\n",
    "        # Compute recall and round to two decimal places\n",
    "        recall = float(round(recall_score(y_true, y_pred, average='macro'), 2))\n",
    "\n",
    "        # Compute F1-score and round to two decimal places\n",
    "        f1 = float(round(f1_score(y_true, y_pred, average='macro'), 2))\n",
    "        \n",
    "        if output:\n",
    "            # Print the rounded metrics for the current language\n",
    "            print(\"Accuracy:\", accuracy)\n",
    "            print(\"Precision:\", precision)\n",
    "            print(\"Recall:\", recall)\n",
    "            print(\"F1-score:\", f1)\n",
    "            print(\"=========================================\")\n",
    "\n",
    "        # Append the metrics to the temporary list\n",
    "        metrics.append(accuracy)\n",
    "        metrics.append(precision)\n",
    "        metrics.append(recall)\n",
    "        metrics.append(f1)\n",
    "\n",
    "        # Append the metrics to the list in the dictionary for the current language\n",
    "        metrics_dict[model_name][language] = metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <ins>CWE Classification (Zero-Shot)</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: CWE-Sys1 + CWE-UserZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file in CodeLlama-7b: ./excel_results/experiment_CWE-Sys1_CWE-UserZ/codellama7b/cwe_classification_results_CodeLlama-7b-Instruct-hf_20240307_043843.xlsx\n",
      "reading file in gpt35: ./excel_results/experiment_CWE-Sys1_CWE-UserZ/gpt35/cwe_classification_results_gpt35-turbo_20240211_180544.xlsx\n",
      "reading file in gpt-4o: ./excel_results/experiment_CWE-Sys1_CWE-UserZ/gpt4o/cwe_classification_results_gpt-4o_20240707_144502.xlsx\n",
      "reading file in CodeLlama-13b: ./excel_results/experiment_CWE-Sys1_CWE-UserZ/codellama13b/cwe_classification_results_CodeLlama-13b-Instruct-hf_20240307_060448.xlsx\n",
      "reading file in gpt4: ./excel_results/experiment_CWE-Sys1_CWE-UserZ/gpt4/cwe_classification_results_gpt4-turbo_20240307_220849.xlsx\n",
      "reading file in gemini: ./excel_results/experiment_CWE-Sys1_CWE-UserZ/gemini1.5pro/cwe_classification_results_gemini-1.5-pro-001_20240709_132658.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Set the base directory\n",
    "base_dir = './excel_results/experiment_CWE-Sys1_CWE-UserZ'\n",
    "# Load the data into data frames\n",
    "df_gpt35, df_gpt4, df_gpt4o, df_codellama7b, df_codellama13b, df_gemini = load_data_to_df_cwe(base_dir)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make a dict to store all metrics for each model\n",
    "# Key is the model name, value is a list to store the metrics\n",
    "metrics_dict = {\n",
    "    \"gpt35\": [],\n",
    "    \"gpt4\": [],\n",
    "    \"gpt4o\": [],\n",
    "    \"CodeLlama-7b\": [],\n",
    "    \"CodeLlama-13b\": [],\n",
    "    \"gemini\": []\n",
    "}\n",
    "\n",
    "# Store the result for each programming language per model in a dictionary.\n",
    "# Key is the model name, value is a dictionary where key is the language and value is a list to store the metrics\n",
    "metrics_per_language_dict = {\n",
    "    \"gpt35\": {},\n",
    "    \"gpt4\": {},\n",
    "    \"gpt4o\": {},\n",
    "    \"CodeLlama-7b\": {},\n",
    "    \"CodeLlama-13b\": {},\n",
    "    \"gemini\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Itrate through each model and calculate the metrics\n",
    "calculate_metrics(df_gpt35, metrics_dict, \"gpt35\")\n",
    "calculate_metrics(df_gpt4, metrics_dict, \"gpt4\")\n",
    "calculate_metrics(df_gpt4o, metrics_dict, \"gpt4o\")\n",
    "calculate_metrics(df_codellama7b, metrics_dict, \"CodeLlama-7b\")\n",
    "calculate_metrics(df_codellama13b, metrics_dict, \"CodeLlama-13b\")\n",
    "calculate_metrics(df_gemini, metrics_dict, \"gemini\")\n",
    "\n",
    "# Calculate the metrics per language for each model\n",
    "calculate_metrics_per_language(df_gpt35, \"gpt35\", metrics_per_language_dict)\n",
    "calculate_metrics_per_language(df_gpt4, \"gpt4\", metrics_per_language_dict)\n",
    "calculate_metrics_per_language(df_gpt4o, \"gpt4o\", metrics_per_language_dict)\n",
    "calculate_metrics_per_language(df_codellama7b, \"CodeLlama-7b\", metrics_per_language_dict)\n",
    "calculate_metrics_per_language(df_codellama13b, \"CodeLlama-13b\", metrics_per_language_dict)\n",
    "calculate_metrics_per_language(df_gemini, \"gemini\", metrics_per_language_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment results: CWE-Sys1 + CWE-UserZ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt35</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-13b</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  precision  recall  f1-score\n",
       "gpt35              0.45       0.14    0.12      0.12\n",
       "gpt4               0.56       0.21    0.18      0.18\n",
       "gpt4o              0.62       0.23    0.19      0.20\n",
       "CodeLlama-7b       0.22       0.12    0.09      0.08\n",
       "CodeLlama-13b      0.33       0.09    0.08      0.08\n",
       "gemini             0.53       0.15    0.13      0.13"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a dataframe to store the metrics, which we can use to analyze the results\n",
    "print(\"Experiment results: CWE-Sys1 + CWE-UserZ\")\n",
    "df_metrics = pd.DataFrame(metrics_dict)\n",
    "df_metrics = df_metrics.T\n",
    "df_metrics.columns = ['accuracy', 'precision', 'recall', 'f1-score']\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: CWE-Sys2 + CWE-UserZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file in CodeLlama-7b: ./excel_results/experiment_CWE-Sys2_CWE-UserZ/codellama7b/cwe_classification_results_CodeLlama-7b-Instruct-hf_20240304_064953.xlsx\n",
      "reading file in gpt35: ./excel_results/experiment_CWE-Sys2_CWE-UserZ/gpt35/cwe_classification_results_gpt35-turbo_20240226_171613.xlsx\n",
      "reading file in gpt-4o: ./excel_results/experiment_CWE-Sys2_CWE-UserZ/gpt4o/cwe_classification_results_gpt-4o_20240708_154331.xlsx\n",
      "reading file in CodeLlama-13b: ./excel_results/experiment_CWE-Sys2_CWE-UserZ/codellama13b/cwe_classification_results_CodeLlama-13b-Instruct-hf_20240304_114823.xlsx\n",
      "reading file in gpt4: ./excel_results/experiment_CWE-Sys2_CWE-UserZ/gpt4/cwe_classification_results_gpt4-turbo_20240307_211950.xlsx\n",
      "reading file in gemini: ./excel_results/experiment_CWE-Sys2_CWE-UserZ/gemini1.5pro/cwe_classification_results_gemini-1.5-pro-001_20240710_153505.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Set the base directory\n",
    "base_dir = './excel_results/experiment_CWE-Sys2_CWE-UserZ'\n",
    "\n",
    "# Load the data into data frames\n",
    "df_gpt35, df_gpt4, df_gpt4o, df_codellama7b, df_codellama13b, df_gemini = load_data_to_df_cwe(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make a dict to store all metrics for each model\n",
    "# Key is the model name, value is a list to store the metrics\n",
    "metrics_dict2 = {\n",
    "    \"gpt35\": [],\n",
    "    \"gpt4\": [],\n",
    "    \"gpt4o\": [],\n",
    "    \"CodeLlama-7b\": [],\n",
    "    \"CodeLlama-13b\": [],\n",
    "    \"gemini\": []\n",
    "}\n",
    "\n",
    "# Store the result for each programming language per model in a dictionary.\n",
    "# Key is the model name, value is a dictionary where key is the language and value is a list to store the metrics\n",
    "metrics_per_language_dict2 = {\n",
    "    \"gpt35\": {},\n",
    "    \"gpt4\": {},\n",
    "    \"gpt4o\": {},\n",
    "    \"CodeLlama-7b\": {},\n",
    "    \"CodeLlama-13b\": {},\n",
    "    \"gemini\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment results: CWE-Sys2 + CWE-UserZ\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt35</th>\n",
       "      <td>0.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-13b</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  precision  recall  f1-score\n",
       "gpt35              0.38       0.14    0.12      0.12\n",
       "gpt4               0.67       0.30    0.23      0.25\n",
       "gpt4o              0.52       0.22    0.19      0.19\n",
       "CodeLlama-7b       0.29       0.08    0.05      0.06\n",
       "CodeLlama-13b      0.25       0.08    0.05      0.05\n",
       "gemini             0.55       0.16    0.15      0.14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Itrate through each model and calculate the metrics\n",
    "calculate_metrics(df_gpt35, metrics_dict2, \"gpt35\")\n",
    "calculate_metrics(df_gpt4, metrics_dict2, \"gpt4\")\n",
    "calculate_metrics(df_gpt4o, metrics_dict2, \"gpt4o\")\n",
    "calculate_metrics(df_codellama7b, metrics_dict2, \"CodeLlama-7b\")\n",
    "calculate_metrics(df_codellama13b, metrics_dict2, \"CodeLlama-13b\")\n",
    "calculate_metrics(df_gemini, metrics_dict2, \"gemini\")\n",
    "\n",
    "# Calculate the metrics per language for each model\n",
    "calculate_metrics_per_language(df_gpt35, \"gpt35\", metrics_per_language_dict2)\n",
    "calculate_metrics_per_language(df_gpt4, \"gpt4\", metrics_per_language_dict2)\n",
    "calculate_metrics_per_language(df_gpt4o, \"gpt4o\", metrics_per_language_dict2)\n",
    "calculate_metrics_per_language(df_codellama7b, \"CodeLlama-7b\", metrics_per_language_dict2)\n",
    "calculate_metrics_per_language(df_codellama13b, \"CodeLlama-13b\", metrics_per_language_dict2)\n",
    "calculate_metrics_per_language(df_gemini, \"gemini\", metrics_per_language_dict2)\n",
    "\n",
    "# Make a dataframe to store the metrics, which we can use to analyze the results\n",
    "print(\"Experiment results: CWE-Sys2 + CWE-UserZ\")\n",
    "df_metrics2 = pd.DataFrame(metrics_dict2)\n",
    "df_metrics2 = df_metrics2.T\n",
    "df_metrics2.columns = ['accuracy', 'precision', 'recall', 'f1-score']\n",
    "df_metrics2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: CWE-Sys1 + CWE-UserF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file in CodeLlama-7b: ./excel_results/experiment_CWE-Sys1_CWE-UserF/codellama7b/cwe_few_shot_classification_results_CodeLlama-7b-Instruct-hf_20240307_044555.xlsx\n",
      "reading file in gpt35: ./excel_results/experiment_CWE-Sys1_CWE-UserF/gpt35/cwe_few_shot_classification_results_gpt35-turbo_20240211_181050.xlsx\n",
      "reading file in gpt-4o: ./excel_results/experiment_CWE-Sys1_CWE-UserF/gpt4o/cwe_few_shot_classification_results_gpt-4o_20240707_171140.xlsx\n",
      "reading file in CodeLlama-13b: ./excel_results/experiment_CWE-Sys1_CWE-UserF/codellama13b/cwe_few_shot_classification_results_CodeLlama-13b-Instruct-hf_20240307_063542.xlsx\n",
      "reading file in gpt4: ./excel_results/experiment_CWE-Sys1_CWE-UserF/gpt4/cwe_few_shot_classification_results_gpt4-turbo_20240311_181323.xlsx\n",
      "reading file in gemini: ./excel_results/experiment_CWE-Sys1_CWE-UserF/gemini1.5pro/cwe_few_shot_classification_results_gemini-1.5-pro-001_20240709_174749.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Set the base directory\n",
    "base_dir = './excel_results/experiment_CWE-Sys1_CWE-UserF'\n",
    "\n",
    "# Load the data into data frames\n",
    "df_gpt35, df_gpt4, df_gpt4o, df_codellama7b, df_codellama13b, df_gemini = load_data_to_df_cwe_few(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make a dict to store all metrics for each model\n",
    "# Key is the model name, value is a list to store the metrics\n",
    "metrics_dict3 = {\n",
    "    \"gpt35\": [],\n",
    "    \"gpt4\": [],\n",
    "    \"gpt4o\": [],\n",
    "    \"CodeLlama-7b\": [],\n",
    "    \"CodeLlama-13b\": [],\n",
    "    \"gemini\": []\n",
    "}\n",
    "\n",
    "# Store the result for each programming language per model in a dictionary.\n",
    "# Key is the model name, value is a dictionary where key is the language and value is a list to store the metrics\n",
    "metrics_per_language_dict3 = {\n",
    "    \"gpt35\": {},\n",
    "    \"gpt4\": {},\n",
    "    \"gpt4o\": {},\n",
    "    \"CodeLlama-7b\": {},\n",
    "    \"CodeLlama-13b\": {},\n",
    "    \"gemini\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment results: CWE-Sys1_CWE-UserF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt35</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-13b</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  precision  recall  f1-score\n",
       "gpt35              0.52       0.34    0.35      0.32\n",
       "gpt4               0.61       0.42    0.52      0.44\n",
       "gpt4o              0.67       0.61    0.69      0.61\n",
       "CodeLlama-7b       0.11       0.13    0.10      0.08\n",
       "CodeLlama-13b      0.23       0.09    0.07      0.07\n",
       "gemini             0.66       0.52    0.56      0.51"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Itrate through each model and calculate the metrics\n",
    "calculate_metrics(df_gpt35, metrics_dict3, \"gpt35\")\n",
    "calculate_metrics(df_gpt4, metrics_dict3, \"gpt4\")\n",
    "calculate_metrics(df_gpt4o, metrics_dict3, \"gpt4o\")\n",
    "calculate_metrics(df_codellama7b, metrics_dict3, \"CodeLlama-7b\")\n",
    "calculate_metrics(df_codellama13b, metrics_dict3, \"CodeLlama-13b\")\n",
    "calculate_metrics(df_gemini, metrics_dict3, \"gemini\")\n",
    "\n",
    "# Calculate the metrics per language for each model\n",
    "calculate_metrics_per_language(df_gpt35, \"gpt35\", metrics_per_language_dict3)\n",
    "calculate_metrics_per_language(df_gpt4, \"gpt4\", metrics_per_language_dict3)\n",
    "calculate_metrics_per_language(df_gpt4o, \"gpt4o\", metrics_per_language_dict3)\n",
    "calculate_metrics_per_language(df_codellama7b, \"CodeLlama-7b\", metrics_per_language_dict3)\n",
    "calculate_metrics_per_language(df_codellama13b, \"CodeLlama-13b\", metrics_per_language_dict3)\n",
    "calculate_metrics_per_language(df_gemini, \"gemini\", metrics_per_language_dict3)\n",
    "\n",
    "# Make a dataframe to store the metrics, which we can use to analyze the results\n",
    "print(\"Experiment results: CWE-Sys1_CWE-UserF\")\n",
    "df_metrics3 = pd.DataFrame(metrics_dict3)\n",
    "df_metrics3 = df_metrics3.T\n",
    "df_metrics3.columns = ['accuracy', 'precision', 'recall', 'f1-score']\n",
    "df_metrics3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4: CWE-Sys2 + CWE-UserF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file in CodeLlama-7b: ./excel_results/experiment_CWE-Sys2_CWE-UserF/codellama7b/cwe_few_shot_classification_results_CodeLlama-7b-Instruct-hf_20240304_075019.xlsx\n",
      "reading file in gpt35: ./excel_results/experiment_CWE-Sys2_CWE-UserF/gpt35/cwe_few_shot_classification_results_gpt35-turbo_20240307_161036.xlsx\n",
      "reading file in gpt-4o: ./excel_results/experiment_CWE-Sys2_CWE-UserF/gpt4o/cwe_few_shot_classification_results_gpt-4o_20240708_165251.xlsx\n",
      "reading file in CodeLlama-13b: ./excel_results/experiment_CWE-Sys2_CWE-UserF/codellama13b/cwe_few_shot_classification_results_CodeLlama-13b-Instruct-hf_20240304_122855.xlsx\n",
      "reading file in gpt4: ./excel_results/experiment_CWE-Sys2_CWE-UserF/gpt4/cwe_few_shot_classification_results_gpt4-turbo_20240317_005738.xlsx\n",
      "reading file in gemini: ./excel_results/experiment_CWE-Sys2_CWE-UserF/gemini1.5pro/cwe_few_shot_classification_results_gemini-1.5-pro-001_20240710_221336.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Set the base directory\n",
    "base_dir = './excel_results/experiment_CWE-Sys2_CWE-UserF'\n",
    "\n",
    "# Load the data into data frames\n",
    "df_gpt35, df_gpt4, df_gpt4o, df_codellama7b, df_codellama13b, df_gemini = load_data_to_df_cwe_few(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make a dict to store all metrics for each model\n",
    "# Key is the model name, value is a list to store the metrics\n",
    "metrics_dict4 = {\n",
    "    \"gpt35\": [],\n",
    "    \"gpt4\": [],\n",
    "    \"gpt4o\": [],\n",
    "    \"CodeLlama-7b\": [],\n",
    "    \"CodeLlama-13b\": [],\n",
    "    \"gemini\": []\n",
    "}\n",
    "\n",
    "# Store the result for each programming language per model in a dictionary.\n",
    "# Key is the model name, value is a dictionary where key is the language and value is a list to store the metrics\n",
    "metrics_per_language_dict4 = {\n",
    "    \"gpt35\": {},\n",
    "    \"gpt4\": {},\n",
    "    \"gpt4o\": {},\n",
    "    \"CodeLlama-7b\": {},\n",
    "    \"CodeLlama-13b\": {},\n",
    "    \"gemini\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment results: CWE-Sys2 CWE-UserF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt35</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-13b</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  precision  recall  f1-score\n",
       "gpt35              0.44       0.24    0.23      0.21\n",
       "gpt4               0.69       0.54    0.58      0.53\n",
       "gpt4o              0.65       0.60    0.74      0.63\n",
       "CodeLlama-7b       0.13       0.11    0.08      0.06\n",
       "CodeLlama-13b      0.21       0.10    0.09      0.07\n",
       "gemini             0.69       0.56    0.52      0.49"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Itrate through each model and calculate the metrics\n",
    "calculate_metrics(df_gpt35, metrics_dict4, \"gpt35\")\n",
    "calculate_metrics(df_gpt4, metrics_dict4, \"gpt4\")\n",
    "calculate_metrics(df_gpt4o, metrics_dict4, \"gpt4o\")\n",
    "calculate_metrics(df_codellama7b, metrics_dict4, \"CodeLlama-7b\")\n",
    "calculate_metrics(df_codellama13b, metrics_dict4, \"CodeLlama-13b\")\n",
    "calculate_metrics(df_gemini, metrics_dict4, \"gemini\")\n",
    "\n",
    "# Calculate the metrics per language for each model\n",
    "calculate_metrics_per_language(df_gpt35, \"gpt35\", metrics_per_language_dict4)\n",
    "calculate_metrics_per_language(df_gpt4, \"gpt4\", metrics_per_language_dict4)\n",
    "calculate_metrics_per_language(df_gpt4o, \"gpt4o\", metrics_per_language_dict4)\n",
    "calculate_metrics_per_language(df_codellama7b, \"CodeLlama-7b\", metrics_per_language_dict4)\n",
    "calculate_metrics_per_language(df_codellama13b, \"CodeLlama-13b\", metrics_per_language_dict4)\n",
    "calculate_metrics_per_language(df_gemini, \"gemini\", metrics_per_language_dict4)\n",
    "\n",
    "# Make a dataframe to store the metrics, which we can use to analyze the results\n",
    "print(\"Experiment results: CWE-Sys2 CWE-UserF\")\n",
    "df_metrics4 = pd.DataFrame(metrics_dict4)\n",
    "df_metrics4 = df_metrics4.T\n",
    "df_metrics4.columns = ['accuracy', 'precision', 'recall', 'f1-score']\n",
    "df_metrics4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gpt35</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4o</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-7b</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CodeLlama-13b</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               accuracy  precision  recall  f1-score  accuracy  precision  \\\n",
       "gpt35              0.45       0.14    0.12      0.12      0.38       0.14   \n",
       "gpt4               0.56       0.21    0.18      0.18      0.67       0.30   \n",
       "gpt4o              0.62       0.23    0.19      0.20      0.52       0.22   \n",
       "CodeLlama-7b       0.22       0.12    0.09      0.08      0.29       0.08   \n",
       "CodeLlama-13b      0.33       0.09    0.08      0.08      0.25       0.08   \n",
       "gemini             0.53       0.15    0.13      0.13      0.55       0.16   \n",
       "\n",
       "               recall  f1-score  accuracy  precision  recall  f1-score  \\\n",
       "gpt35            0.12      0.12      0.52       0.34    0.35      0.32   \n",
       "gpt4             0.23      0.25      0.61       0.42    0.52      0.44   \n",
       "gpt4o            0.19      0.19      0.67       0.61    0.69      0.61   \n",
       "CodeLlama-7b     0.05      0.06      0.11       0.13    0.10      0.08   \n",
       "CodeLlama-13b    0.05      0.05      0.23       0.09    0.07      0.07   \n",
       "gemini           0.15      0.14      0.66       0.52    0.56      0.51   \n",
       "\n",
       "               accuracy  precision  recall  f1-score  \n",
       "gpt35              0.44       0.24    0.23      0.21  \n",
       "gpt4               0.69       0.54    0.58      0.53  \n",
       "gpt4o              0.65       0.60    0.74      0.63  \n",
       "CodeLlama-7b       0.13       0.11    0.08      0.06  \n",
       "CodeLlama-13b      0.21       0.10    0.09      0.07  \n",
       "gemini             0.69       0.56    0.52      0.49  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If there is not a directory named 'experiment_results_cwe' create one\n",
    "if not os.path.exists('experiment_results_cwe'):\n",
    "    os.makedirs('experiment_results_cwe')\n",
    "\n",
    "# Concatenate the dataframes horizontally to compare the results\n",
    "df_metrics_all = pd.concat([df_metrics, df_metrics2, df_metrics3, df_metrics4], axis=1)\n",
    "# Save the metrics to an Excel file\n",
    "df_metrics_all.to_csv('experiment_results_cwe/experiment_results_cwe.csv')\n",
    "df_metrics_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the results of the experiments per programming language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_dataframe_matches_dict(df_dict, original_dict):\n",
    "    \"\"\"\n",
    "    Compare dataframes to corresponding values in an original dictionary and report mismatches.\n",
    "    \n",
    "    This function iterates through a dictionary of dataframes, comparing the values of specific columns\n",
    "    ('Accuracy', 'Precision', 'Recall', 'F1-Score') in each dataframe to the values stored in an original dictionary.\n",
    "    If there are discrepancies, it reports the mismatches.\n",
    "    \"\"\"\n",
    "\n",
    "    mismatches = []\n",
    "    \n",
    "    for language, df in df_dict.items():\n",
    "        for index, row in df.iterrows():\n",
    "            model = row['Model']\n",
    "            if model not in original_dict:\n",
    "                mismatches.append(f\"Model {model} not found in original dictionary for {language}\")\n",
    "                continue\n",
    "            \n",
    "            original_values = original_dict[model][language]\n",
    "            df_values = row[['Accuracy', 'Precision', 'Recall', 'F1-Score']].values\n",
    "            \n",
    "            # Check if original_values and df_values are not the same\n",
    "            if not np.array_equal(original_values, df_values):\n",
    "                mismatches.append(f\"Values for {model} do not match for {language}\")\n",
    "\n",
    "    if mismatches:\n",
    "        print(\"Mismatches found:\")\n",
    "        for mismatch in mismatches:\n",
    "            print(mismatch)\n",
    "    else:\n",
    "        print(\"All values match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['python', 'c', 'cpp', 'javascript', 'java']\n",
    "all_dicts = [metrics_per_language_dict, metrics_per_language_dict2, metrics_per_language_dict3, metrics_per_language_dict4]\n",
    "\n",
    "# Initialize a list to hold dictionaries of dataframes for each metrics dictionary\n",
    "all_dfs = []\n",
    "\n",
    "for m_dict in all_dicts:\n",
    "    # Initialize a dictionary to hold dataframes for each language\n",
    "    dfs = {language: pd.DataFrame() for language in languages}\n",
    "    for language in languages:\n",
    "        data = []\n",
    "        for model, metrics in m_dict.items():\n",
    "            data.append([model] + metrics[language])\n",
    "        dfs[language] = pd.DataFrame(data, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "    all_dfs.append(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values match!\n",
      "All values match!\n",
      "All values match!\n",
      "All values match!\n"
     ]
    }
   ],
   "source": [
    "# Assuming all_dfs[0] corresponds to metrics_per_language_dict\n",
    "test_dataframe_matches_dict(all_dfs[0], metrics_per_language_dict)\n",
    "test_dataframe_matches_dict(all_dfs[1], metrics_per_language_dict2)\n",
    "test_dataframe_matches_dict(all_dfs[2], metrics_per_language_dict3)\n",
    "test_dataframe_matches_dict(all_dfs[3], metrics_per_language_dict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Model' column from all_dfs[1] to all_dfs[3] since as long as the all_dfs[0] has the 'Model' column\n",
    "for language in languages:\n",
    "    all_dfs[1][language] = all_dfs[1][language].drop(columns='Model')\n",
    "    all_dfs[2][language] = all_dfs[2][language].drop(columns='Model')\n",
    "    all_dfs[3][language] = all_dfs[3][language].drop(columns='Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE experiment results for Python:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt35</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4o</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CodeLlama-7b</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CodeLlama-13b</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision  Recall  F1-Score  Accuracy  Precision  \\\n",
       "0          gpt35      0.39       0.25    0.21      0.22      0.38       0.22   \n",
       "1           gpt4      0.61       0.43    0.42      0.40      0.66       0.42   \n",
       "2          gpt4o      0.54       0.36    0.26      0.27      0.51       0.35   \n",
       "3   CodeLlama-7b      0.13       0.09    0.07      0.07      0.26       0.16   \n",
       "4  CodeLlama-13b      0.29       0.16    0.15      0.14      0.24       0.10   \n",
       "5         gemini      0.54       0.24    0.21      0.20      0.50       0.25   \n",
       "\n",
       "   Recall  F1-Score  Accuracy  Precision  Recall  F1-Score  Accuracy  \\\n",
       "0    0.20      0.20      0.47       0.27    0.32      0.28      0.46   \n",
       "1    0.35      0.36      0.51       0.53    0.54      0.48      0.59   \n",
       "2    0.31      0.30      0.53       0.58    0.52      0.51      0.58   \n",
       "3    0.10      0.11      0.01       0.00    0.02      0.01      0.11   \n",
       "4    0.09      0.08      0.21       0.17    0.13      0.11      0.18   \n",
       "5    0.22      0.22      0.67       0.61    0.64      0.58      0.71   \n",
       "\n",
       "   Precision  Recall  F1-Score  \n",
       "0       0.32    0.30      0.29  \n",
       "1       0.59    0.55      0.52  \n",
       "2       0.60    0.65      0.60  \n",
       "3       0.15    0.14      0.11  \n",
       "4       0.17    0.18      0.12  \n",
       "5       0.72    0.62      0.62  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_dfs[0], metrics_per_language_dict -> CWE-Sys1 + CWE-UserZ\n",
    "# all_dfs[1], metrics_per_language_dict2 -> CWE-Sys2 + CWE-UserZ\n",
    "# all_dfs[2], metrics_per_language_dict3 -> CWE-Sys1 + CWE-UserF\n",
    "# all_dfs[3], metrics_per_language_dict4 -> VD-Sys2 + CWE-UserF\n",
    "\n",
    "# Create a folder to store all csv files\n",
    "if not os.path.exists('experiment_results_cwe'):\n",
    "    os.makedirs('experiment_results_cwe')\n",
    "\n",
    "# Concatenate the dataframes horizontally to compare the results for Python\n",
    "df_python = pd.concat([all_dfs[0]['python'], all_dfs[1]['python'], all_dfs[2]['python'], all_dfs[3]['python']], axis=1)\n",
    "# Save the metrics to an CSV file\n",
    "df_python.to_csv('experiment_results_cwe/python.csv')\n",
    "print(\"CWE experiment results for Python:\")\n",
    "df_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE experiment results for C\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt35</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4o</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CodeLlama-7b</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CodeLlama-13b</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision  Recall  F1-Score  Accuracy  Precision  \\\n",
       "0          gpt35      0.43       0.21    0.20      0.19      0.34       0.20   \n",
       "1           gpt4      0.60       0.38    0.37      0.36      0.70       0.46   \n",
       "2          gpt4o      0.61       0.37    0.31      0.32      0.48       0.33   \n",
       "3   CodeLlama-7b      0.22       0.19    0.15      0.15      0.19       0.08   \n",
       "4  CodeLlama-13b      0.31       0.18    0.15      0.15      0.22       0.15   \n",
       "5         gemini      0.40       0.22    0.17      0.18      0.53       0.29   \n",
       "\n",
       "   Recall  F1-Score  Accuracy  Precision  Recall  F1-Score  Accuracy  \\\n",
       "0    0.19      0.19      0.44       0.46    0.43      0.39      0.41   \n",
       "1    0.42      0.43      0.65       0.56    0.66      0.58      0.70   \n",
       "2    0.28      0.29      0.66       0.64    0.74      0.65      0.60   \n",
       "3    0.06      0.07      0.16       0.12    0.20      0.13      0.15   \n",
       "4    0.12      0.12      0.22       0.14    0.13      0.12      0.24   \n",
       "5    0.26      0.26      0.67       0.57    0.60      0.56      0.70   \n",
       "\n",
       "   Precision  Recall  F1-Score  \n",
       "0       0.32    0.33      0.31  \n",
       "1       0.67    0.69      0.66  \n",
       "2       0.62    0.76      0.64  \n",
       "3       0.12    0.17      0.12  \n",
       "4       0.20    0.21      0.18  \n",
       "5       0.67    0.62      0.61  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dataframes horizontally to compare the results for C\n",
    "df_c = pd.concat([all_dfs[0]['c'], all_dfs[1]['c'], all_dfs[2]['c'], all_dfs[3]['c']], axis=1)\n",
    "# Save the metrics to an CSV file\n",
    "df_c.to_csv('experiment_results_cwe/c.csv')\n",
    "print(\"CWE experiment results for C\")\n",
    "df_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for C++\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE experiment results for C++\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt35</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4o</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CodeLlama-7b</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CodeLlama-13b</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision  Recall  F1-Score  Accuracy  Precision  \\\n",
       "0          gpt35      0.43       0.21    0.19      0.19      0.36       0.25   \n",
       "1           gpt4      0.53       0.39    0.39      0.37      0.62       0.44   \n",
       "2          gpt4o      0.65       0.42    0.39      0.39      0.51       0.40   \n",
       "3   CodeLlama-7b      0.33       0.17    0.15      0.14      0.36       0.18   \n",
       "4  CodeLlama-13b      0.29       0.18    0.15      0.15      0.25       0.12   \n",
       "5         gemini      0.58       0.32    0.28      0.29      0.62       0.32   \n",
       "\n",
       "   Recall  F1-Score  Accuracy  Precision  Recall  F1-Score  Accuracy  \\\n",
       "0    0.22      0.22      0.54       0.43    0.46      0.41      0.43   \n",
       "1    0.42      0.41      0.64       0.59    0.68      0.60      0.68   \n",
       "2    0.38      0.36      0.65       0.61    0.75      0.64      0.60   \n",
       "3    0.12      0.14      0.15       0.17    0.18      0.16      0.11   \n",
       "4    0.11      0.10      0.26       0.22    0.20      0.19      0.24   \n",
       "5    0.30      0.30      0.60       0.45    0.40      0.39      0.64   \n",
       "\n",
       "   Precision  Recall  F1-Score  \n",
       "0       0.32    0.32      0.29  \n",
       "1       0.57    0.66      0.57  \n",
       "2       0.59    0.76      0.62  \n",
       "3       0.09    0.13      0.09  \n",
       "4       0.27    0.22      0.19  \n",
       "5       0.45    0.42      0.41  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dataframes horizontally to compare the results for C++\n",
    "df_cpp = pd.concat([all_dfs[0]['cpp'], all_dfs[1]['cpp'], all_dfs[2]['cpp'], all_dfs[3]['cpp']], axis=1)\n",
    "# Save the metrics to an CSV file\n",
    "df_cpp.to_csv('experiment_results_cwe/cpp.csv')\n",
    "print(\"CWE experiment results for C++\")\n",
    "df_cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE experiment results for JavaScript\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt35</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4o</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CodeLlama-7b</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CodeLlama-13b</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision  Recall  F1-Score  Accuracy  Precision  \\\n",
       "0          gpt35      0.53       0.25    0.26      0.24      0.47       0.27   \n",
       "1           gpt4      0.52       0.28    0.28      0.26      0.76       0.52   \n",
       "2          gpt4o      0.69       0.37    0.42      0.38      0.53       0.29   \n",
       "3   CodeLlama-7b      0.21       0.16    0.20      0.17      0.36       0.25   \n",
       "4  CodeLlama-13b      0.41       0.26    0.30      0.26      0.38       0.27   \n",
       "5         gemini      0.66       0.31    0.35      0.33      0.59       0.27   \n",
       "\n",
       "   Recall  F1-Score  Accuracy  Precision  Recall  F1-Score  Accuracy  \\\n",
       "0    0.27      0.25      0.64       0.42    0.50      0.44      0.50   \n",
       "1    0.52      0.50      0.53       0.52    0.64      0.53      0.79   \n",
       "2    0.29      0.27      0.74       0.59    0.62      0.59      0.69   \n",
       "3    0.25      0.22      0.14       0.13    0.19      0.13      0.14   \n",
       "4    0.27      0.23      0.17       0.08    0.07      0.06      0.14   \n",
       "5    0.28      0.26      0.72       0.60    0.73      0.63      0.76   \n",
       "\n",
       "   Precision  Recall  F1-Score  \n",
       "0       0.41    0.40      0.37  \n",
       "1       0.68    0.71      0.66  \n",
       "2       0.53    0.61      0.55  \n",
       "3       0.05    0.12      0.06  \n",
       "4       0.13    0.11      0.09  \n",
       "5       0.52    0.52      0.49  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dataframes horizontally to compare the results for JavaScript\n",
    "df_js = pd.concat([all_dfs[0]['javascript'], all_dfs[1]['javascript'], all_dfs[2]['javascript'], all_dfs[3]['javascript']], axis=1)\n",
    "# Save the metrics to an CSV file\n",
    "df_js.to_csv('experiment_results_cwe/javascript.csv')\n",
    "print(\"CWE experiment results for JavaScript\")\n",
    "df_js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result for Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWE experiment results for Java\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt35</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt4</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt4o</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CodeLlama-7b</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CodeLlama-13b</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gemini</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Precision  Recall  F1-Score  Accuracy  Precision  \\\n",
       "0          gpt35      0.46       0.19    0.16      0.16      0.39       0.22   \n",
       "1           gpt4      0.51       0.29    0.23      0.25      0.63       0.42   \n",
       "2          gpt4o      0.62       0.35    0.30      0.31      0.58       0.36   \n",
       "3   CodeLlama-7b      0.21       0.14    0.10      0.10      0.31       0.11   \n",
       "4  CodeLlama-13b      0.36       0.17    0.18      0.17      0.23       0.16   \n",
       "5         gemini      0.51       0.28    0.26      0.26      0.51       0.29   \n",
       "\n",
       "   Recall  F1-Score  Accuracy  Precision  Recall  F1-Score  Accuracy  \\\n",
       "0    0.18      0.19      0.55       0.28    0.31      0.29      0.42   \n",
       "1    0.35      0.37      0.68       0.49    0.55      0.50      0.69   \n",
       "2    0.32      0.32      0.76       0.59    0.63      0.59      0.77   \n",
       "3    0.11      0.10      0.10       0.07    0.11      0.07      0.14   \n",
       "4    0.16      0.14      0.29       0.14    0.15      0.12      0.24   \n",
       "5    0.26      0.26      0.65       0.59    0.69      0.59      0.65   \n",
       "\n",
       "   Precision  Recall  F1-Score  \n",
       "0       0.22    0.23      0.21  \n",
       "1       0.48    0.52      0.48  \n",
       "2       0.63    0.68      0.64  \n",
       "3       0.09    0.13      0.08  \n",
       "4       0.15    0.12      0.12  \n",
       "5       0.52    0.60      0.52  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the dataframes horizontally to compare the results for Java\n",
    "df_java = pd.concat([all_dfs[0]['java'], all_dfs[1]['java'], all_dfs[2]['java'], all_dfs[3]['java']], axis=1)\n",
    "# Save the metrics to an CSV file\n",
    "df_java.to_csv('experiment_results_cwe/java.csv')\n",
    "print(\"CWE experiment results for Java\")\n",
    "df_java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tennis_detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
